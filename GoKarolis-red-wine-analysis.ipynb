{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nfrom sklearn.preprocessing import LabelEncoder #, StandardScaler","execution_count":17,"outputs":[{"output_type":"stream","text":"/kaggle/input/red-wine-quality-cortez-et-al-2009/winequality-red.csv\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# wine_data == data\ndata = pd.read_csv('../input/red-wine-quality-cortez-et-al-2009/winequality-red.csv')","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Exploratory data analysis"},{"metadata":{},"cell_type":"markdown","source":"# **1.1. Shape**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"(1599, 12)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **1.2. Descriptive statistics**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(20)","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"    fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n0             7.4             0.700         0.00             1.9      0.076   \n1             7.8             0.880         0.00             2.6      0.098   \n2             7.8             0.760         0.04             2.3      0.092   \n3            11.2             0.280         0.56             1.9      0.075   \n4             7.4             0.700         0.00             1.9      0.076   \n5             7.4             0.660         0.00             1.8      0.075   \n6             7.9             0.600         0.06             1.6      0.069   \n7             7.3             0.650         0.00             1.2      0.065   \n8             7.8             0.580         0.02             2.0      0.073   \n9             7.5             0.500         0.36             6.1      0.071   \n10            6.7             0.580         0.08             1.8      0.097   \n11            7.5             0.500         0.36             6.1      0.071   \n12            5.6             0.615         0.00             1.6      0.089   \n13            7.8             0.610         0.29             1.6      0.114   \n14            8.9             0.620         0.18             3.8      0.176   \n15            8.9             0.620         0.19             3.9      0.170   \n16            8.5             0.280         0.56             1.8      0.092   \n17            8.1             0.560         0.28             1.7      0.368   \n18            7.4             0.590         0.08             4.4      0.086   \n19            7.9             0.320         0.51             1.8      0.341   \n\n    free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n0                  11.0                  34.0   0.9978  3.51       0.56   \n1                  25.0                  67.0   0.9968  3.20       0.68   \n2                  15.0                  54.0   0.9970  3.26       0.65   \n3                  17.0                  60.0   0.9980  3.16       0.58   \n4                  11.0                  34.0   0.9978  3.51       0.56   \n5                  13.0                  40.0   0.9978  3.51       0.56   \n6                  15.0                  59.0   0.9964  3.30       0.46   \n7                  15.0                  21.0   0.9946  3.39       0.47   \n8                   9.0                  18.0   0.9968  3.36       0.57   \n9                  17.0                 102.0   0.9978  3.35       0.80   \n10                 15.0                  65.0   0.9959  3.28       0.54   \n11                 17.0                 102.0   0.9978  3.35       0.80   \n12                 16.0                  59.0   0.9943  3.58       0.52   \n13                  9.0                  29.0   0.9974  3.26       1.56   \n14                 52.0                 145.0   0.9986  3.16       0.88   \n15                 51.0                 148.0   0.9986  3.17       0.93   \n16                 35.0                 103.0   0.9969  3.30       0.75   \n17                 16.0                  56.0   0.9968  3.11       1.28   \n18                  6.0                  29.0   0.9974  3.38       0.50   \n19                 17.0                  56.0   0.9969  3.04       1.08   \n\n    alcohol  quality  \n0       9.4        5  \n1       9.8        5  \n2       9.8        5  \n3       9.8        6  \n4       9.4        5  \n5       9.4        5  \n6       9.4        5  \n7      10.0        7  \n8       9.5        7  \n9      10.5        5  \n10      9.2        5  \n11     10.5        5  \n12      9.9        5  \n13      9.1        5  \n14      9.2        5  \n15      9.2        5  \n16     10.5        7  \n17      9.3        5  \n18      9.0        4  \n19      9.2        6  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fixed acidity</th>\n      <th>volatile acidity</th>\n      <th>citric acid</th>\n      <th>residual sugar</th>\n      <th>chlorides</th>\n      <th>free sulfur dioxide</th>\n      <th>total sulfur dioxide</th>\n      <th>density</th>\n      <th>pH</th>\n      <th>sulphates</th>\n      <th>alcohol</th>\n      <th>quality</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7.4</td>\n      <td>0.700</td>\n      <td>0.00</td>\n      <td>1.9</td>\n      <td>0.076</td>\n      <td>11.0</td>\n      <td>34.0</td>\n      <td>0.9978</td>\n      <td>3.51</td>\n      <td>0.56</td>\n      <td>9.4</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7.8</td>\n      <td>0.880</td>\n      <td>0.00</td>\n      <td>2.6</td>\n      <td>0.098</td>\n      <td>25.0</td>\n      <td>67.0</td>\n      <td>0.9968</td>\n      <td>3.20</td>\n      <td>0.68</td>\n      <td>9.8</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7.8</td>\n      <td>0.760</td>\n      <td>0.04</td>\n      <td>2.3</td>\n      <td>0.092</td>\n      <td>15.0</td>\n      <td>54.0</td>\n      <td>0.9970</td>\n      <td>3.26</td>\n      <td>0.65</td>\n      <td>9.8</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>11.2</td>\n      <td>0.280</td>\n      <td>0.56</td>\n      <td>1.9</td>\n      <td>0.075</td>\n      <td>17.0</td>\n      <td>60.0</td>\n      <td>0.9980</td>\n      <td>3.16</td>\n      <td>0.58</td>\n      <td>9.8</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7.4</td>\n      <td>0.700</td>\n      <td>0.00</td>\n      <td>1.9</td>\n      <td>0.076</td>\n      <td>11.0</td>\n      <td>34.0</td>\n      <td>0.9978</td>\n      <td>3.51</td>\n      <td>0.56</td>\n      <td>9.4</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>7.4</td>\n      <td>0.660</td>\n      <td>0.00</td>\n      <td>1.8</td>\n      <td>0.075</td>\n      <td>13.0</td>\n      <td>40.0</td>\n      <td>0.9978</td>\n      <td>3.51</td>\n      <td>0.56</td>\n      <td>9.4</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7.9</td>\n      <td>0.600</td>\n      <td>0.06</td>\n      <td>1.6</td>\n      <td>0.069</td>\n      <td>15.0</td>\n      <td>59.0</td>\n      <td>0.9964</td>\n      <td>3.30</td>\n      <td>0.46</td>\n      <td>9.4</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7.3</td>\n      <td>0.650</td>\n      <td>0.00</td>\n      <td>1.2</td>\n      <td>0.065</td>\n      <td>15.0</td>\n      <td>21.0</td>\n      <td>0.9946</td>\n      <td>3.39</td>\n      <td>0.47</td>\n      <td>10.0</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>7.8</td>\n      <td>0.580</td>\n      <td>0.02</td>\n      <td>2.0</td>\n      <td>0.073</td>\n      <td>9.0</td>\n      <td>18.0</td>\n      <td>0.9968</td>\n      <td>3.36</td>\n      <td>0.57</td>\n      <td>9.5</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>7.5</td>\n      <td>0.500</td>\n      <td>0.36</td>\n      <td>6.1</td>\n      <td>0.071</td>\n      <td>17.0</td>\n      <td>102.0</td>\n      <td>0.9978</td>\n      <td>3.35</td>\n      <td>0.80</td>\n      <td>10.5</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>6.7</td>\n      <td>0.580</td>\n      <td>0.08</td>\n      <td>1.8</td>\n      <td>0.097</td>\n      <td>15.0</td>\n      <td>65.0</td>\n      <td>0.9959</td>\n      <td>3.28</td>\n      <td>0.54</td>\n      <td>9.2</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>7.5</td>\n      <td>0.500</td>\n      <td>0.36</td>\n      <td>6.1</td>\n      <td>0.071</td>\n      <td>17.0</td>\n      <td>102.0</td>\n      <td>0.9978</td>\n      <td>3.35</td>\n      <td>0.80</td>\n      <td>10.5</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>5.6</td>\n      <td>0.615</td>\n      <td>0.00</td>\n      <td>1.6</td>\n      <td>0.089</td>\n      <td>16.0</td>\n      <td>59.0</td>\n      <td>0.9943</td>\n      <td>3.58</td>\n      <td>0.52</td>\n      <td>9.9</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>7.8</td>\n      <td>0.610</td>\n      <td>0.29</td>\n      <td>1.6</td>\n      <td>0.114</td>\n      <td>9.0</td>\n      <td>29.0</td>\n      <td>0.9974</td>\n      <td>3.26</td>\n      <td>1.56</td>\n      <td>9.1</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>8.9</td>\n      <td>0.620</td>\n      <td>0.18</td>\n      <td>3.8</td>\n      <td>0.176</td>\n      <td>52.0</td>\n      <td>145.0</td>\n      <td>0.9986</td>\n      <td>3.16</td>\n      <td>0.88</td>\n      <td>9.2</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>8.9</td>\n      <td>0.620</td>\n      <td>0.19</td>\n      <td>3.9</td>\n      <td>0.170</td>\n      <td>51.0</td>\n      <td>148.0</td>\n      <td>0.9986</td>\n      <td>3.17</td>\n      <td>0.93</td>\n      <td>9.2</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>8.5</td>\n      <td>0.280</td>\n      <td>0.56</td>\n      <td>1.8</td>\n      <td>0.092</td>\n      <td>35.0</td>\n      <td>103.0</td>\n      <td>0.9969</td>\n      <td>3.30</td>\n      <td>0.75</td>\n      <td>10.5</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>8.1</td>\n      <td>0.560</td>\n      <td>0.28</td>\n      <td>1.7</td>\n      <td>0.368</td>\n      <td>16.0</td>\n      <td>56.0</td>\n      <td>0.9968</td>\n      <td>3.11</td>\n      <td>1.28</td>\n      <td>9.3</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>7.4</td>\n      <td>0.590</td>\n      <td>0.08</td>\n      <td>4.4</td>\n      <td>0.086</td>\n      <td>6.0</td>\n      <td>29.0</td>\n      <td>0.9974</td>\n      <td>3.38</td>\n      <td>0.50</td>\n      <td>9.0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>7.9</td>\n      <td>0.320</td>\n      <td>0.51</td>\n      <td>1.8</td>\n      <td>0.341</td>\n      <td>17.0</td>\n      <td>56.0</td>\n      <td>0.9969</td>\n      <td>3.04</td>\n      <td>1.08</td>\n      <td>9.2</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"       fixed acidity  volatile acidity  citric acid  residual sugar  \\\ncount    1599.000000       1599.000000  1599.000000     1599.000000   \nmean        8.319637          0.527821     0.270976        2.538806   \nstd         1.741096          0.179060     0.194801        1.409928   \nmin         4.600000          0.120000     0.000000        0.900000   \n25%         7.100000          0.390000     0.090000        1.900000   \n50%         7.900000          0.520000     0.260000        2.200000   \n75%         9.200000          0.640000     0.420000        2.600000   \nmax        15.900000          1.580000     1.000000       15.500000   \n\n         chlorides  free sulfur dioxide  total sulfur dioxide      density  \\\ncount  1599.000000          1599.000000           1599.000000  1599.000000   \nmean      0.087467            15.874922             46.467792     0.996747   \nstd       0.047065            10.460157             32.895324     0.001887   \nmin       0.012000             1.000000              6.000000     0.990070   \n25%       0.070000             7.000000             22.000000     0.995600   \n50%       0.079000            14.000000             38.000000     0.996750   \n75%       0.090000            21.000000             62.000000     0.997835   \nmax       0.611000            72.000000            289.000000     1.003690   \n\n                pH    sulphates      alcohol      quality  \ncount  1599.000000  1599.000000  1599.000000  1599.000000  \nmean      3.311113     0.658149    10.422983     5.636023  \nstd       0.154386     0.169507     1.065668     0.807569  \nmin       2.740000     0.330000     8.400000     3.000000  \n25%       3.210000     0.550000     9.500000     5.000000  \n50%       3.310000     0.620000    10.200000     6.000000  \n75%       3.400000     0.730000    11.100000     6.000000  \nmax       4.010000     2.000000    14.900000     8.000000  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fixed acidity</th>\n      <th>volatile acidity</th>\n      <th>citric acid</th>\n      <th>residual sugar</th>\n      <th>chlorides</th>\n      <th>free sulfur dioxide</th>\n      <th>total sulfur dioxide</th>\n      <th>density</th>\n      <th>pH</th>\n      <th>sulphates</th>\n      <th>alcohol</th>\n      <th>quality</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>1599.000000</td>\n      <td>1599.000000</td>\n      <td>1599.000000</td>\n      <td>1599.000000</td>\n      <td>1599.000000</td>\n      <td>1599.000000</td>\n      <td>1599.000000</td>\n      <td>1599.000000</td>\n      <td>1599.000000</td>\n      <td>1599.000000</td>\n      <td>1599.000000</td>\n      <td>1599.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>8.319637</td>\n      <td>0.527821</td>\n      <td>0.270976</td>\n      <td>2.538806</td>\n      <td>0.087467</td>\n      <td>15.874922</td>\n      <td>46.467792</td>\n      <td>0.996747</td>\n      <td>3.311113</td>\n      <td>0.658149</td>\n      <td>10.422983</td>\n      <td>5.636023</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1.741096</td>\n      <td>0.179060</td>\n      <td>0.194801</td>\n      <td>1.409928</td>\n      <td>0.047065</td>\n      <td>10.460157</td>\n      <td>32.895324</td>\n      <td>0.001887</td>\n      <td>0.154386</td>\n      <td>0.169507</td>\n      <td>1.065668</td>\n      <td>0.807569</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>4.600000</td>\n      <td>0.120000</td>\n      <td>0.000000</td>\n      <td>0.900000</td>\n      <td>0.012000</td>\n      <td>1.000000</td>\n      <td>6.000000</td>\n      <td>0.990070</td>\n      <td>2.740000</td>\n      <td>0.330000</td>\n      <td>8.400000</td>\n      <td>3.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>7.100000</td>\n      <td>0.390000</td>\n      <td>0.090000</td>\n      <td>1.900000</td>\n      <td>0.070000</td>\n      <td>7.000000</td>\n      <td>22.000000</td>\n      <td>0.995600</td>\n      <td>3.210000</td>\n      <td>0.550000</td>\n      <td>9.500000</td>\n      <td>5.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>7.900000</td>\n      <td>0.520000</td>\n      <td>0.260000</td>\n      <td>2.200000</td>\n      <td>0.079000</td>\n      <td>14.000000</td>\n      <td>38.000000</td>\n      <td>0.996750</td>\n      <td>3.310000</td>\n      <td>0.620000</td>\n      <td>10.200000</td>\n      <td>6.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>9.200000</td>\n      <td>0.640000</td>\n      <td>0.420000</td>\n      <td>2.600000</td>\n      <td>0.090000</td>\n      <td>21.000000</td>\n      <td>62.000000</td>\n      <td>0.997835</td>\n      <td>3.400000</td>\n      <td>0.730000</td>\n      <td>11.100000</td>\n      <td>6.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>15.900000</td>\n      <td>1.580000</td>\n      <td>1.000000</td>\n      <td>15.500000</td>\n      <td>0.611000</td>\n      <td>72.000000</td>\n      <td>289.000000</td>\n      <td>1.003690</td>\n      <td>4.010000</td>\n      <td>2.000000</td>\n      <td>14.900000</td>\n      <td>8.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":15,"outputs":[{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"fixed acidity           0\nvolatile acidity        0\ncitric acid             0\nresidual sugar          0\nchlorides               0\nfree sulfur dioxide     0\ntotal sulfur dioxide    0\ndensity                 0\npH                      0\nsulphates               0\nalcohol                 0\nquality                 0\ndtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def display_all(data):\n    with pd.option_context(\"display.max_rows\", 10000, \"display.max_columns\", 10000): \n        display(data)\n\ndisplay_all(data.describe(include='all').T)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **1.3. Data types and more information about the data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.dtypes","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"fixed acidity           float64\nvolatile acidity        float64\ncitric acid             float64\nresidual sugar          float64\nchlorides               float64\nfree sulfur dioxide     float64\ntotal sulfur dioxide    float64\ndensity                 float64\npH                      float64\nsulphates               float64\nalcohol                 float64\nquality                   int64\ndtype: object"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['quality'].value_counts().sort_index(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Pre-processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"bins = (2, 6.5, 8)\ngroup_names = ['bad', 'good']\ndata['quality'] = pd.cut(data['quality'], bins = bins, labels = group_names)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_quality = LabelEncoder()\n\ndata['quality'] = label_quality.fit_transform(data['quality'])\n\nX = data.drop('quality', axis = 1)\ny = data['quality']\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train test split"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)","execution_count":8,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'train_test_split' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-1c879298acc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def split_vals(a,n): return a[:n], a[n:]\nn_valid = 200\nn_trn = len(data)-n_valid\nX_train, X_valid = split_vals(df_trn, n_trn)\ny_train, y_valid = split_vals(y_trn, n_trn)\nraw_train, raw_valid = split_vals(df_raw, n_trn)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Out-of-bag score**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"y = data.quality\nX = data\nX_train,X_test,y_train,y_test = train_test_split(data.drop('quality',axis=1),data['quality'],test_size=0.25,random_state=42)\n\n\nm = RandomForestRegressor(n_estimators=40, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"valid_fraction = 0.1\nvalid_size = int(len(data) * valid_fraction)\n\nx_train = data[:-2 * valid_size]\nx_valid = data[-2 * valid_size:-valid_size]\nx_test = data[-valid_size:]\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data.drop('quality',axis=1)\ny = data['quality']\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)","execution_count":12,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# k-Nearest Neighbour"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\n\nknn = KNeighborsClassifier()\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\n\nprint(classification_report(y_test,y_pred))\n\n\n\n\"\"\"neighbors = np.arange(1,9)\ntrain_accuracy =np.empty(len(neighbors))\nvalid_accuracy = np.empty(len(neighbors))\n\nfor i,k in enumerate(neighbors):\n    #Setup a knn classifier with k neighbors\n    knn = KNeighborsClassifier(n_neighbors=k)\n    \n    #Fit the model\n    knn.fit(X_train, y_test)\n    \n    #Compute accuracy on the training set\n    train_accuracy[i] = knn.score(X_train, y_train)\n    \n    #Compute accuracy on the test set\n    test_accuracy[i] = knn.score(X_test, y_test)   \"\"\"","execution_count":13,"outputs":[{"output_type":"stream","text":"              precision    recall  f1-score   support\n\n         bad       0.88      0.97      0.92       273\n        good       0.53      0.21      0.30        47\n\n    accuracy                           0.86       320\n   macro avg       0.70      0.59      0.61       320\nweighted avg       0.83      0.86      0.83       320\n\n","name":"stdout"},{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"'neighbors = np.arange(1,9)\\ntrain_accuracy =np.empty(len(neighbors))\\nvalid_accuracy = np.empty(len(neighbors))\\n\\nfor i,k in enumerate(neighbors):\\n    #Setup a knn classifier with k neighbors\\n    knn = KNeighborsClassifier(n_neighbors=k)\\n    \\n    #Fit the model\\n    knn.fit(X_train, y_test)\\n    \\n    #Compute accuracy on the training set\\n    train_accuracy[i] = knn.score(X_train, y_train)\\n    \\n    #Compute accuracy on the test set\\n    test_accuracy[i] = knn.score(X_test, y_test)   '"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\nX = data.drop('quality',axis=1).values\ny = data['quality'].values\n\nx_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n\n\"\"\"neighbors = np.arange(1,9)\ntrain_accuracy =np.empty(len(neighbors))\nvalid_accuracy = np.empty(len(neighbors))\n\nfor i,k in enumerate(neighbors):\n    #Setup a knn classifier with k neighbors\n    knn = KNeighborsClassifier(n_neighbors=k)\n    \n    #Fit the model\n    knn.fit(x_train, y_test)\n    \n    #Compute accuracy on the training set\n    train_accuracy[i] = knn.score(x_train, y_train)\n    \n    #Compute accuracy on the test set\n    test_accuracy[i] = knn.score(x_test, y_test) \n    \n\n\n#X = data.data\n#y = data.target\n\n#y = data.quality\n#data_features = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'total sulfur dioxide']\n#X = data[data_features]\n\n#x_train,x_test,y_train,y_test=train_test_split(data.drop('quality',axis=1),data['quality'],test_size=0.25,random_state=42)\n\n\nk = 1\n#Train Model and Predict  \nneigh = KNeighborsClassifier(n_neighbors = k).fit(x_train,y)\n\nyhat = neigh.predict(x_valid)\nyhat[0:5]\n\n# Jaccard index\n\nprint(\"Train set Accuracy: \", metrics.accuracy_score(y_train, neigh.predict(x_train)))\nprint(\"Test set Accuracy: \", metrics.accuracy_score(y_valid, yhat))\n\"\"\"","execution_count":14,"outputs":[{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"'neighbors = np.arange(1,9)\\ntrain_accuracy =np.empty(len(neighbors))\\nvalid_accuracy = np.empty(len(neighbors))\\n\\nfor i,k in enumerate(neighbors):\\n    #Setup a knn classifier with k neighbors\\n    knn = KNeighborsClassifier(n_neighbors=k)\\n    \\n    #Fit the model\\n    knn.fit(x_train, y_test)\\n    \\n    #Compute accuracy on the training set\\n    train_accuracy[i] = knn.score(x_train, y_train)\\n    \\n    #Compute accuracy on the test set\\n    test_accuracy[i] = knn.score(x_test, y_test) \\n    \\n\\n\\n#X = data.data\\n#y = data.target\\n\\n#y = data.quality\\n#data_features = [\\'fixed acidity\\', \\'volatile acidity\\', \\'citric acid\\', \\'residual sugar\\', \\'total sulfur dioxide\\']\\n#X = data[data_features]\\n\\n#x_train,x_test,y_train,y_test=train_test_split(data.drop(\\'quality\\',axis=1),data[\\'quality\\'],test_size=0.25,random_state=42)\\n\\n\\nk = 1\\n#Train Model and Predict  \\nneigh = KNeighborsClassifier(n_neighbors = k).fit(x_train,y)\\n\\nyhat = neigh.predict(x_valid)\\nyhat[0:5]\\n\\n# Jaccard index\\n\\nprint(\"Train set Accuracy: \", metrics.accuracy_score(y_train, neigh.predict(x_train)))\\nprint(\"Test set Accuracy: \", metrics.accuracy_score(y_valid, yhat))\\n'"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Decision Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"#from sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_absolute_error\n\n\ny = data.quality\nwine_features = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'total sulfur dioxide']\nX = data[wine_features]\n\n#train_X, val_X, train_y, val_y = train_test_split(X, y,random_state = 0)\n\ndef get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n    model.fit(train_X, train_y)\n    preds_val = model.predict(val_X)\n    mae = mean_absolute_error(val_y, preds_val)\n    return(mae)\n\ncandidate_max_leaf_nodes = [5, 25, 50, 100, 250, 500]\n\nscores = {leaf_size: get_mae(leaf_size, train_X, val_X, train_y, val_y) for leaf_size in candidate_max_leaf_nodes}\nbest_tree_size = min(scores, key=scores.get)\n\nfinal_model = DecisionTreeRegressor(max_leaf_nodes=best_tree_size, random_state=1)\n\nfinal_model.fit(X, y)\n\n\n\n\nval_predictions = final_model.predict(val_X)\nval_mae = mean_absolute_error(val_predictions, val_y)\nprint(\"Validation MAE: {:,.0f}\".format(val_mae))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier()\nrf.fit(X_train, y_train)\ny_pred=rf.predict(X_test)\n\nprint(classification_report(y_test,y_pred))","execution_count":19,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'RandomForestClassifier' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-3acfc2b05af0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'RandomForestClassifier' is not defined"]}]},{"metadata":{},"cell_type":"markdown","source":"# Logistic regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression()\nlr.fit(X_train, y_train)\nlr_predict = lr.predict(X_test)\n\nprint(classification_report(y_test,lr_predict))","execution_count":18,"outputs":[{"output_type":"stream","text":"              precision    recall  f1-score   support\n\n         bad       0.88      0.98      0.93       273\n        good       0.62      0.21      0.32        47\n\n    accuracy                           0.87       320\n   macro avg       0.75      0.60      0.62       320\nweighted avg       0.84      0.87      0.84       320\n\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"Requirements\n    Do exploratory data analysis.\n    Do feature engineering.\n    Do feature preprocessing.\n    Build a machine learning model.\n    Explore your model results.\nBonus Points\n    Move your final model to a Python module.\n    Compare multiple machine learning models.\n    Do hyperparameter tuning.\n    Deploy the model on your own machine.\n    Deploy the model on external service."},{"metadata":{},"cell_type":"markdown","source":"\n \n\n7. If missing values are present then how to impute them?\n \n\nFor various scenarios, while dealing with data you will come across real-world data which will have missing values like nan values, -, blanks. The basic approach to deal with such a situation is to drop/ remove the entire row or column which contains missing values. But dropping is not advisable because there will be a loss of data as well which can result in important parts of the data being removed. So, to deal with such things there are different methods used to impute the missing values. \n\n \n\nThere are two ways by which missing values can be imputed: first is called univariate imputation and the other one is multivariate imputation. \n\n \n\nUnivariate imputation is a type of imputation which imputes missing values considering only the non-missing values in that feature dimension. (e.g. impute.SimpleImputer). \n\n \n\nOn the other hand, a multivariate imputer imputes the missing values considering all available features dimension.(e.g. impute.IterativeImputer). You can read more about imputing missing values here.\n\n \n\n \n\n8. Encoding categorical features \n \n\nOften it is seen that we do not have continuous values in our features. There are sometimes categorical values. And the system cannot understand such values so there is a need to convert them to continuous numerical values. \n\n \n\nAs seen in the below iris data frame we have classes as categorical features which are - ‘Iris-setosa’, ‘Iris-versicolor’, ‘Iris-virginica’.\n\n \n\nHow to encode them?\n \n \nUnique values in class column of the iris data-set which contains categorical values.\n\n \n\nThere are several different techniques which are used to encode categorical values which are stated below:\n\n \n\na) LabelEncoder() - It is a function present in the scikit- learn library of python which is used to convert categorical values in numerical values.\n\n \n\nHands-on implementation of encoding using LabelEncoder.\n\n \n\nHere we have imported LabelEncoder from sklearn.preprocessing followed by initialising of the object through which we will use the label encoder. We have made an object called “LE”. Then we have transformed our class column by using the LE.fit_transform function & printed the transformed class which is now [0,1,2]. It has given the values to Iris-setosa - 0, Iris-versicolor  - 1 , Iris-virginica - 2.\n\n \n\nb) get_dummies() - Converts categorical features into dummy variables. \n\n \n\nc) OneHotEncoder() - Array-like of integers or strings is the required input for this encoder. The features are encoded using a one-hot encoding scheme. The result is a binary column for each category and reverts a sparse matrix. \n\n \n\n \n\n9. Standardization of data \n \n\nStandardization of data is a major important step that is required for machine learning algorithms to give good results. There are different scaling functions present in the preprocessing module of sci-kit learn. If data is not scaled and is passed to the algorithm the result might be wrong due to wrongly distributed data.\n\n \n\nWhy is it important to scale the data?\n \n\nIt is usually seen that we ignore checking the shape of the distribution and change the data to be centered. That is done by removing the mean values of each column and then scaling it by dividing non-constant columns by their standard deviation.\n\n \n\nDifferent functions that are used by algorithms to learn to assume that all the desired features are centered as zero and also their variance is in the same structure. If any of the features have a higher proportion than all other features it may dominate the function for learning algorithm and does not allow learning from other features as required. \n\n \n\na) Scale :  present in the pre-processing module gives a fast and effective way to do this operation on a single array-like data:\n\n \n\nHand-on implementation of scaling using scale.\n\n \n\nX_scaled has now unit variance and zero mean as you can see in the below image.\n\n \n\nScaled data has zero mean and unit variance.\n\n \n\nb) The pre-processing module also has different other classes like StandardScaler that are used in scaling the data that is converting the mean to be zero and standard deviation to be unit on training data which can be further used in test data as well. Such class can also be used in building pipelines also.\n\n \n\nThe code implementation of standard scaler is shown below.\n\n \n\nCode implementation of Standard scaler.\n\n \n\nc) Scaling features to a range:  There are other methods also to scale data within a respective range that is a min values and max value. It mainly ranges between 0 and 1. You can use MinMaxScaler or MaxAbsScaler for scaling the data respectively.\n\n \n\nd) Scaling sparse data: Centering the scatter data would result in knock-down of sparsity structure of data thus it is not advisable to do. MinMaxScaler and MaxAbs scaler were introduced to scale the sparse data. Scalar often accepts both CSR (Compressed Sparse Rows) & also CSC (Compressed Sparse Columns). If there is any other different sparse input then it is converted to Compressed Sparse Rows. To take care of the memory it is advisable to convert it in CSR and CSC representation.\n\n \n\ne) Scaling data with presence of outliers : If the data has outliers in it then scaling that sort of data using mean and variance is not a good approach. You can use robust_scale & Robust_Scaler as drop in substitution.\n\n \n\n10. Normalization of data \n \n\nIt is the process of scaling each sample to have a unit standard. These types of techniques are much more effective if you are computing the similarity between different pairs of samples or to use a quadratic form like a dot product.This is the base of models used in text classifications. \n\n \n\nThere is a function in the pre-processing module that is normalized which provides a good way to execute such operations on single array-like data by using l1 or l2 standards. Implementation of normalizing data using normalize is shown in the below image.\n\n \n\nCode to normalize the data using Normalize function.\n\n \n\nPre-processing module also has another class that is called a normalizer that executes the similar operations using the transformer API. This class can also be used in the initial stage of pipeline. Implementation of normalizer is shown below in the image.\n\n \n\nCode to normalize the data using normalizer function.\n\n \n\nIf you want to look for code implementation of EDA discussed above you can refer to the GitHub link here. It contains a jupyter file and both the datasets which are used. \n"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}